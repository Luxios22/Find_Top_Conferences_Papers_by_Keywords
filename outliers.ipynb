{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1011bc9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T03:50:12.035508Z",
     "start_time": "2021-10-15T03:50:11.910416Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b9eea7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T06:18:04.233411Z",
     "start_time": "2021-10-15T06:18:04.203374Z"
    }
   },
   "outputs": [],
   "source": [
    "from googlesearch import search, get_random_user_agent\n",
    "from difflib import SequenceMatcher\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def similarity(a, b):\n",
    "    match = SequenceMatcher(None, a, b)\n",
    "    return match.ratio()\n",
    "\n",
    "\n",
    "def parse_url(url, title):\n",
    "    if \".pdf\" in url.split('/')[-1]:\n",
    "        return url\n",
    "    if \"proceedings.neurips.cc\" in url and url.endswith(\".html\"):\n",
    "        return url[:-13].replace('hash', 'file') + \"Paper.pdf\"\n",
    "    if \"doi.org\" or 'ieeexplore.ieee.org' in url:\n",
    "        try:\n",
    "            thepage = requests.get(\"https://sci-hub.ee/\" + url)\n",
    "            soup = BeautifulSoup(thepage.text, \"html.parser\")\n",
    "            if soup.find('id=pdf'):\n",
    "                pdf_link = soup.find(id='pdf').get(\"src\")\n",
    "            elif soup.find(type=\"application/x-google-chrome-pdf\"):\n",
    "                pdf_link = soup.find(type=\"application/x-google-chrome-pdf\").get(\"src\")\n",
    "            if \"http\" not in pdf_link:\n",
    "                return \"https:\" + pdf_link\n",
    "            else:\n",
    "                return pdf_link\n",
    "        except Exception as e:\n",
    "            print(\"Sci-Hub Exception:\", e)\n",
    "            print(url)\n",
    "            time.sleep(5)\n",
    "    if \"openaccess.thecvf.com\" in url and url.endswith(\".html\"):\n",
    "#         return url[:-5].replace('html', 'papers') + \".pdf\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "            searched_title = soup.find(id=\"papertitle\").text.strip().lower()\n",
    "            if similarity(title, searched_title) > 0.6:\n",
    "                return \"https://openaccess.thecvf.com/\" + soup.find('a', string='pdf').get('href').replace(\"../\", \"\")\n",
    "            else:\n",
    "                print(f\"OPENACCESS NOT MATCHED: {title} -- {searched_title}\")\n",
    "        except Exception as e:\n",
    "            print(\"OPENACCESS Exception:\", e)\n",
    "            time.sleep(5)\n",
    "    if 'arxiv.org/abs' in url:\n",
    "#         return url.replace('abs', 'pdf')+'.pdf'\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "            searched_title = ' '.join(soup.title.text.lower().split()[1:])\n",
    "            if similarity(title, searched_title) > 0.8:\n",
    "                return url.replace('abs', 'pdf')+'.pdf'\n",
    "            else:\n",
    "                print(f\"ARXIV NOT MATCHED: {title} -- {searched_title}\")\n",
    "        except Exception as e:\n",
    "            print(\"ARXIV Exception:\", e)\n",
    "            time.sleep(5)\n",
    "    if 'aaai.org' in url or 'index.php/AAAI' in url:\n",
    "        try:\n",
    "            soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "            searched_title = soup.find(class_=\"page_title\").text.strip().lower()\n",
    "            if similarity(title, searched_title) > 0.6:\n",
    "                return soup.find(class_=\"obj_galley_link pdf\").get(\"href\")\n",
    "            else:\n",
    "                print(f\"AAAI NOT MATCHED: {title} -- {searched_title}\")\n",
    "        except AttributeError as e:\n",
    "            thepage = requests.get(url.replace('view', 'viewPaper'))\n",
    "            soup = BeautifulSoup(thepage.text, \"html.parser\")\n",
    "            searched_title = soup.find(id=\"title\").text.strip().lower()\n",
    "            if similarity(title, searched_title) > 0.6:\n",
    "                print(\"AAAI: find it again\")\n",
    "                return soup.find(\"meta\", attrs= {'name': 'citation_pdf_url'}).get(\"content\")\n",
    "            else:\n",
    "                print(f\"AAAI NOT MATCHED: {title} -- {searched_title}\")\n",
    "        except Exception as e:\n",
    "            print(\"AAAI Exception:\", e)\n",
    "            time.sleep(5)\n",
    "\n",
    "\n",
    "def search_pdf_link(title, conf, url):\n",
    "    time.sleep(1)\n",
    "    pdf_link = None\n",
    "    user_agent = get_random_user_agent()\n",
    "    pdf_link = parse_url(url, title)\n",
    "    if pdf_link:\n",
    "        return pdf_link\n",
    "    search_string = ' '.join([title, conf, \"pdf\"])\n",
    "    for j in search(search_string, num=3, stop=3, pause=2.0, user_agent=user_agent):\n",
    "        pdf_link = parse_url(j, title)\n",
    "        if pdf_link:\n",
    "            return pdf_link\n",
    "                 \n",
    "    print(f\"NOT SEARCHED: {title}\")\n",
    "    print(conf, url)\n",
    "    return pdf_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df88f5f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T05:51:56.717702Z",
     "start_time": "2021-10-15T05:51:56.708730Z"
    }
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# NTU, NUS, SUTD, SMU, SIT, SUSS\n",
    "sg_universities = {\n",
    "    'NTU':[\"Nanyang Technological University\", \"ntu.edu.sg\", \"NTU\"],\n",
    "    'NUS':[\"National University of Singapore\", \"nus.edu.sg\", \"NUS\"],\n",
    "    'SUTD': [\"Singapore University of Technology and Design\", \"sutd.edu.sg\", \"SUTD\"],\n",
    "    'SMU':[\"Singapore Management University\", \"smu.edu.sg\", \"SMU\"],\n",
    "    'SIT': [\"Singapore Institute of Technology\",\"SingaporeTech.edu.sg\", \"singaporetech.edu.sg\"],\n",
    "    'SUSS': [\"Singapore University of Social Sciences\", \"suss.edu.sg\", \"SUSS\"]\n",
    "    }\n",
    "search_list = []\n",
    "for uni, keywords in sg_universities.items():\n",
    "    search_list += keywords\n",
    "\n",
    "def get_key (dict, value):\n",
    "    return [k for k, v in dict.items() if value in v]\n",
    "\n",
    "#path = 'https://arxiv.org/pdf/1807.01440.pdf'\n",
    "def find_sg_university(path):\n",
    "    print(\"path:\", path)\n",
    "    uni = None\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            data = requests.get(path).content\n",
    "            doc = fitz.open(stream=data, filetype=\"pdf\")\n",
    "            break\n",
    "        except RuntimeError:\n",
    "            with open(\"pdfs/outlier.pdf\", 'wb') as f:\n",
    "                f.write(data)\n",
    "            doc = fitz.open(\"pdfs/outlier.pdf\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(repr(e), \"the pdf can not be parsed.\")\n",
    "    page = doc.loadPage(0)\n",
    "    for word in search_list:\n",
    "        x = page.search_for(word) #list\n",
    "        hit = ''\n",
    "        if len(x) > 0:\n",
    "            for i in range(len(x)):\n",
    "                hit_item = page.get_textbox(x[i])\n",
    "                print(\"hit_item:\", hit_item)\n",
    "                hit += hit_item\n",
    "                if hit in search_list:\n",
    "                    print(\"hit:\", hit)\n",
    "                    uni = get_key(sg_universities, hit)[0]\n",
    "                    print(\"uni:\", uni)\n",
    "                    return uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: https://sci-hub.se/downloads/2021-06-16/c8/peng2020.pdf#navpanes=0&view=FitH\n",
      "path: https://sci-hub.se/downloads/2021-06-10/cf/gao2020.pdf#navpanes=0&view=FitH\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "keyword = 'person re-id'\n",
    "header = ['conference', 'year', 'author', 'title', 'booktitle', 'bibtex', 'electronic_edition']\n",
    "\n",
    "def outliers_length():\n",
    "    with open(\"outliers.csv\", 'r') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        outliers_length = len(list(csvreader))\n",
    "        print(\"outliers_length:\", outliers_length)\n",
    "        return outliers_length\n",
    "\n",
    "# while(outliers_length()>0):\n",
    "with open(f'sg_{keyword}_papers.csv', 'a+', encoding='UTF8', newline='') as f1, open('outliers.csv', 'r', encoding='UTF8', newline='') as f2, open('outliers_bak.csv', 'w', encoding='UTF8', newline='') as f3:\n",
    "    reader = csv.reader(f2)\n",
    "    header = next(reader)\n",
    "    writer1 = csv.writer(f1)\n",
    "    writer2 = csv.writer(f3)\n",
    "    writer2.writerow(header)\n",
    "    for paper in reader:\n",
    "        conf, year, author, title, booktitle, biburl, ee_link = paper\n",
    "        try:\n",
    "            arxiv_pdf_link = search_pdf_link(title, conf, ee_link)\n",
    "            if arxiv_pdf_link: \n",
    "                uni = find_sg_university(arxiv_pdf_link)\n",
    "                if uni:\n",
    "                    info = author + title + booktitle \n",
    "                    writer1.writerow([uni, year, conf] + [info] + [biburl, ee_link])\n",
    "            else:\n",
    "                writer2.writerow(paper)\n",
    "        except Exception as e:\n",
    "            print(\"search pdf failed:\", e)\n",
    "            writer2.writerow(paper)\n",
    "with open('outliers.csv', 'w', encoding='UTF8', newline='') as f1, open('outliers_bak.csv', 'r', encoding='UTF8', newline='') as f2:\n",
    "    reader = csv.reader(f2)\n",
    "    writer = csv.writer(f1)\n",
    "    for row in reader:\n",
    "        writer.writerow(row)\n",
    "#     time.sleep(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = requests.get(\"https://ojs.aaai.org/index.php/AAAI/article/view/3762/3640\").content\n",
    "# soup = BeautifulSoup(data, 'html.parser')\n",
    "# soup.find(\"meta\", attrs= {'name': 'citation_pdf_url'}).get(\"content\")\n",
    "doc = fitz.open(stream=data, filetype=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thepage = requests.get(\"https://sci-hub.ee/https://doi.org/10.1609/aaai.v33i01.33018738\")\n",
    "soup = BeautifulSoup(thepage.text, \"html.parser\")\n",
    "type(soup.find(type=\"application/x-google-chrome-pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f771bcd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T06:50:10.957373Z",
     "start_time": "2021-10-15T06:50:06.523155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ieeexplore.ieee.org/document/8237389\n",
      "https://www.researchgate.net/publication/322060407_SHaPE_A_Novel_Graph_Theoretic_Algorithm_for_Making_Consensus-Based_Decisions_in_Person_Re-identification_Systems\n",
      "https://www.semanticscholar.org/paper/SHaPE%3A-A-Novel-Graph-Theoretic-Algorithm-for-Making-Barman-Shah/18ab9be9af94f2bf4d3828161ffb232d1462526a\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "for j in search(\"SHaPE: A Novel Graph Theoretic Algorithm for Making Consensus-Based Decisions in Person Re-identification Systems\", tld='co.id', num=3, stop=3, pause=2.0, user_agent=get_random_user_agent()):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "409b02b9f956488176ee49db827ea02a6920053b4d4ac2eb31a5e3fd0390af38"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('scrape': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
